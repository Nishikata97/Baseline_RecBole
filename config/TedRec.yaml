n_layers: 2
n_heads: 2
hidden_size: 128 # 300
inner_size: 512 # 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: 'CE'

plm_suffix: qwen_pca128_emb.npy # feat1CLS
plm_size: 128 # 768
adaptor_dropout_prob: 0.2
adaptor_layers: [128,128] # [768,300]
temperature: 0.07
n_exps: 8
